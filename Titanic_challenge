2020-05-08
By adding "Fare","Embarked" to  RandomForestClassifier(n_estimators=100, max_depth=4, random_state=1) 
And using 'test_data = test_data.fillna(method='ffill')' to fill in test.Fare only
I have received my best result on the first day:
0.78947

2020-05-09
Attempt 01:
1) test_data['Fare'] - another way to fill
test_data[test_data['Fare'].isna()] # checking who was going 'for free'
test_data[150:154] #check his neighbours
My guess is that Mr Thomas had not expensive fare, otherwise the price would be properly recorderd. Filling his ticket with minimal price
test_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].min()) #which appears to be Zero

2) test_data['Age'] - adding age becasue it may influence physicall fitness and survival rate
test_data[test_data['Age'].isna()] # checking who has no age
# I do not see system in age omission, fill it with mean
test_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())
train_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())
test_data['Age'].hist(); # checking the histogram

3) checking disbalance
print('In our list of', train_data['Survived'].count(), 'passengers', 'only', train_data['Survived'].sum(), 'survived')
print('Disbalance is not strong - survival share',"{:3.1%}".format(train_data['Survived'].sum()/train_data['Survived'].count()) )

In our list of 891 passengers only 342 survived
Disbalance is not strong - survival share 38.4%

# selecting features
features = ["Pclass","Age" ,"Sex", "SibSp", "Parch","Fare","Embarked"]

# work with categories, avoiding dummy trap
y = train_data["Survived"]
X = pd.get_dummies(train_data[features], drop_first=True)
X_test = pd.get_dummies(test_data[features], drop_first=True)

#Let us apply scaller to data
from sklearn.preprocessing import StandardScaler
#numerical fields
numeric = ['Pclass', 'Age', 'SibSp','Parch', 'Fare']
# change scale of numeric fields
scaler = StandardScaler()
scaler.fit(X)
X = scaler.transform(X) 
X_test = scaler.transform(X_test)

# importing set of metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score  
from sklearn.metrics import precision_score  
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
# 30 percent of train data are going into valid dataset
features_train, features_valid, target_train, target_valid = train_test_split(
    X, y, test_size=0.3, random_state=12345)

print('Train dataset has',  len(features_train), 'entries')
Train dataset has 623 entries
print('Validation dataset has',  len(features_valid), 'entries')
Validation dataset has 268 entries

Looking for params before submission - iterative procedure

for depth in range(1, 16, 1):
    model = RandomForestClassifier(max_depth=depth, n_estimators = 100, random_state=1234)
    model.fit(features_train, target_train)
    predicted_valid = model.predict(features_valid)
    print("max_depth =", depth,  ": ", end='')
    print("F1 for RandomForestClassifier after up-sampling:", end='')
    print("F1:", f1_score(target_valid, predicted_valid))
    print('Area under curve for ROC: ', end='')
    probabilities_valid = model.predict_proba(features_valid)
    probabilities_one_valid = probabilities_valid[:, 1]
    auc_roc = roc_auc_score(target_valid,probabilities_one_valid)
    print(auc_roc)

max_depth = 1 : F1 for RandomForestClassifier after up-sampling:F1: 0.5033112582781457
Area under curve for ROC: 0.8197687728937728
max_depth = 2 : F1 for RandomForestClassifier after up-sampling:F1: 0.6242774566473989
Area under curve for ROC: 0.8234603937728937
max_depth = 3 : F1 for RandomForestClassifier after up-sampling:F1: 0.64
Area under curve for ROC: 0.8262076465201467
max_depth = 4 : F1 for RandomForestClassifier after up-sampling:F1: 0.6775956284153005
Area under curve for ROC: 0.8323603479853481
max_depth = 5 : F1 for RandomForestClassifier after up-sampling:F1: 0.6739130434782609
Area under curve for ROC: 0.8314445970695971
max_depth = 6 : F1 for RandomForestClassifier after up-sampling:F1: 0.6739130434782609
Area under curve for ROC: 0.8349931318681318
max_depth = 7 : F1 for RandomForestClassifier after up-sampling:F1: 0.6844919786096257
Area under curve for ROC: 0.8422619047619049
max_depth = 8 : F1 for RandomForestClassifier after up-sampling:F1: 0.7046632124352331
Area under curve for ROC: 0.8395718864468864
max_depth = 9 : F1 for RandomForestClassifier after up-sampling:F1: 0.7070707070707071
Area under curve for ROC: 0.8432062728937729
max_depth = 10 : F1 for RandomForestClassifier after up-sampling:F1: 0.6995073891625615
Area under curve for ROC: 0.8465544871794871
max_depth = 11 : F1 for RandomForestClassifier after up-sampling:F1: 0.7087378640776699
Area under curve for ROC: 0.8401728479853481
max_depth = 12 : F1 for RandomForestClassifier after up-sampling:F1: 0.7264150943396227
Area under curve for ROC: 0.8413461538461537
max_depth = 13 : F1 for RandomForestClassifier after up-sampling:F1: 0.7047619047619047
Area under curve for ROC: 0.8402586996336996
max_depth = 14 : F1 for RandomForestClassifier after up-sampling:F1: 0.7081339712918661
Area under curve for ROC: 0.8369104853479853
max_depth = 15 : F1 for RandomForestClassifier after up-sampling:F1: 0.6981132075471698
Area under curve for ROC: 0.8338770604395604
  
 Best case is max_depth = 12 : F1 for RandomForestClassifier after up-sampling:F1: 0.7264150943396227 Area under curve for ROC: 0.8413461538461537
 
model = RandomForestClassifier(max_depth=12, n_estimators = 100, random_state=1234)
model.fit(features_train, target_train)
predicted_valid = model.predict(features_valid)

print('Confusion matrix:')
print(confusion_matrix(target_valid, predicted_valid))

Confusion matrix:
[[133  23]
 [ 35  77]]

Test predictions and output as per kaggle requirement

model = RandomForestClassifier(n_estimators=100, max_depth=12, random_state=1234)
#model = GradientBoostingClassifier(n_estimators=100, max_depth=15, random_state=1)

model.fit(X, y)
predictions = model.predict(X_test)

output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})
output.to_csv('my_submission.csv', index=False)
print("Your submission was successfully saved!")
--------------------------------------------------------------------------------------------
Your submission scored 0.77033, which is not an improvement of your best score. Keep trying!
--------------------------------------------------------------------------------------------

Attempt02
Training model with best parameters in the same reduced features_train, target_train

best_model = RandomForestClassifier(max_depth=12, n_estimators = 100, random_state=1234)
best_model.fit(features_train, target_train)
predicted_valid = best_model.predict(features_valid)

print('Confusion matrix:')
print(confusion_matrix(target_valid, predicted_valid))

predictions = best_model.predict(X_test) #using best model to predict
-----------------------------------------------------------------------------------------------
Your submission scored 0.76555, which is not an improvement of your best score. Keep trying
---------------------------------------------------------------------

Attempt03:
#from the 1st two attempts we can see that our model becomes over-trained at depth=12,
# so let us limit this parameter
for depth in range(1, 8, 1):
    model = GradientBoostingClassifier(max_depth=depth, n_estimators = 100, random_state=1234)
    model.fit(features_train, target_train)
    predicted_valid = model.predict(features_valid)
    print("max_depth =", depth,  ": ", end='')
    print("F1 for GradientBoostingClassifier after up-sampling:", end='')
    print("F1:", f1_score(target_valid, predicted_valid))
    print('Area under curve for ROC: ', end='')
    probabilities_valid = model.predict_proba(features_valid)
    probabilities_one_valid = probabilities_valid[:, 1]
    auc_roc = roc_auc_score(target_valid,probabilities_one_valid)
    print(auc_roc)
    
max_depth = 1 : F1 for GradientBoostingClassifier after up-sampling:F1: 0.7184466019417476
Area under curve for ROC: 0.8286401098901098
max_depth = 2 : F1 for GradientBoostingClassifier after up-sampling:F1: 0.6774193548387096
Area under curve for ROC: 0.8266082875457875
max_depth = 3 : F1 for GradientBoostingClassifier after up-sampling:F1: 0.6910994764397905
Area under curve for ROC: 0.8357371794871795
max_depth = 4 : F1 for GradientBoostingClassifier after up-sampling:F1: 0.71
Area under curve for ROC: 0.8484146062271063
max_depth = 5 : F1 for GradientBoostingClassifier after up-sampling:F1: 0.7093596059113301
Area under curve for ROC: 0.8276098901098901
max_depth = 6 : F1 for GradientBoostingClassifier after up-sampling:F1: 0.6889952153110048
Area under curve for ROC: 0.822687728937729
max_depth = 7 : F1 for GradientBoostingClassifier after up-sampling:F1: 0.6919431279620853
Area under curve for ROC: 0.814417353479853


max_depth = 4 : F1 for GradientBoostingClassifier after up-sampling:F1: 0.71 Area under curve for ROC: 0.8484146062271063

#
model = GradientBoostingClassifier(n_estimators=100, max_depth=4, random_state=1)

model.fit(X, y)
predictions = model.predict(X_test) #using best model to predict

output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})
output.to_csv('my_submission.csv', index=False)
print("Your submission was successfully saved!")

-------------------------------------------
our submission scored 0.75598, which is not an improvement of your best score. Keep trying!

Gradient boosting is no good for me, will try other methods

Attempt 4
model = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=1234)
Your submission scored 0.76076, which is not an improvement of your best score. Keep trying!

Attempt 4

was the same RandomForestClassifier(n_estimators=100, max_depth=4, random_state=1234) but without age
Your submission scored 0.77511, which is not an improvement of your best score. Keep trying!

So ages do not improve forecast

---------------------------------------------------------------------------

Than we have studied the history of rescue and outcomes for different groups


train_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean()) # filling gaps in age by mean value

# our prvious iteration showed that 'straightforward' application of age is not helping

# let us add child feature for children of age less than 15. 
# We know from history that childred and woman with children of the 1st-2nd class
# were invited  first to boats


ch=0 #setting counter of children to 0

for i in range(len(train_data['Age'])):
    if train_data.loc[i,'Age']<15:
        train_data.loc[i,'Child'] = 1.
        ch +=1
    else:
        train_data.loc[i,'Child'] = 0.

print('Number of children:', ch)

# probability to survive fo different ports/class and gender
pd.pivot_table(train_data, values='Survived', index=['Pclass', 'Sex'],columns=['Embarked'], aggfunc=np.mean)

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
# Draw a heatmap with the numeric values in each cell
f, ax = plt.subplots(figsize=(9, 6))
surv = pd.pivot_table(train_data, values='Survived', index=['Pclass', 'Sex'],columns=['Embarked'], aggfunc=np.mean)
sns.heatmap(surv, annot=True, linewidths=.5, ax=ax);

We can see that almost all women of the 1st-2nd classes survived. For the third class situation varies depending on poart of boarding. That means all these 3 parameters are valuable for the training. (different ports/class and gender)






Exact number of siblings/parents is not meaningfull info. They first saved lonely women or woman with children. Let us make generalization :






# make it boolean
for i in range(len(train_data['Parch'])):
    if train_data.loc[i,'Parch'] !=0:
        train_data.loc[i,'Parch'] = train_data.loc[i,'Parch'] / train_data.loc[i,'Parch']
    if train_data.loc[i,'SibSp'] !=0:
        train_data.loc[i,'SibSp'] = train_data.loc[i,'SibSp'] / train_data.loc[i,'SibSp']
        
# Draw a heatmap with chances to survive for childred and spose/children relationship
f, ax = plt.subplots(figsize=(9, 6))
surv = pd.pivot_table(train_data, values='Survived', index=['Parch', 'SibSp'],columns=['Child'], aggfunc=np.mean)
sns.heatmap(surv, annot=True, linewidths=.5, ax=ax);        


Adding 3 set rescue category men/woman/child - 'Res_category'





for i in range(len(train_data['Child'])):
    if train_data.loc[i,'Child'] ==1:
        train_data.loc[i,'Res_category'] = 'child'
        
    elif train_data.loc[i,'Sex'] == 'male':
        train_data.loc[i,'Res_category'] = 'male'
    
    elif train_data.loc[i,'Sex'] == 'female':
        train_data.loc[i,'Res_category'] = 'female'

# selecting features
features = ["Pclass","Res_category", "SibSp", "Parch","Embarked"]


2020-05-10 Day03
children_treshhold = 14. # set child/nochild limit
for i in range(len(train_data['Name'])): #remove older than children_trashhold
    if ((train_data.loc[i,'Age']>children_treshhold)&(train_data.loc[i,'Child'] == 1)):
        train_data.loc[i,'Child'] = 0.
        ch -=1
print('Number of children:', ch)

for i in range(len(predictions)):
    if (predictions[i] == 1)&(test_data.loc[i,'Child']==1):
        print("Cild. surviver # ",i, test_data.loc[i,:])
        
test_data.query('(PassengerId == 897) | (PassengerId == 1122)') # survived in RL
PassengerId	Pclass	Name	Sex	Age	SibSp	Parch	Ticket	Fare	Cabin	Embarked	Child	Res_category
5	897	3	Svensson, Mr. Johan Cervin	male	14.0	0.0	0.0	7538	9.225	NaN	S	0.0	male
230	1122	2	Sweet, Mr. George Frederick	male	14.0	0.0	0.0	220845	65.000	NaN	S	0.0	male





test_data.query('(PassengerId == 1141) | (PassengerId == 1231)') # died in RL :(
PassengerId	Pclass	Name	Sex	Age	SibSp	Parch	Ticket	Fare	Cabin	Embarked	Child	Res_category
249	1141	3	Khalil, Mrs. Betros (Zahie Maria" Elias)"	female	NaN	1.0	0.0	2660	14.4542	NaN	C	0.0	female
339	1231	3	Betros, Master. Seman	male	NaN	0.0	0.0	2622	7.2292	NaN	C	1.0	child

train_data['Age'] = train_data['Age'].fillna(-1) #filling with indicative
# Age_void may mean other social profile or difficulties to speak english (and to understand/follow rescue orders)
av=0 #setting counter of children to 0

for i in range(len(train_data['Name'])):
      
    if ((train_data.loc[i,'Age']== -1)):
        train_data.loc[i,'Age_void'] = 1.
        av +=1
    else:
        train_data.loc[i,'Age_void'] = 0.
    

print('Number of Age_void:', av)

train_data.query('(Age_void == 1) and (Child == 0)')


2020-05-11 0.79425 was the best result of the previous day

let us start to reduce dimensions now and adding polinomial features
instead of Pclass and Res category/Sex intoducina Crew_invites_to_boat feature:
1) everyone have chance to be invited to the boat = 1.
2) 1st class have the maximum chance, 3rd is the lowest *(4 - Pclass)
3) Children had the highest chance *3/than females *2/than males *1

for i in range(len(test_data['Res_category'])):
    if test_data.loc[i,'Res_category'] =='child':
        test_data.loc[i,'Crew_invites_to_boat'] = 3 * (4-test_data.loc[i,'Pclass'])
        
    elif test_data.loc[i,'Res_category'] =='female':
        test_data.loc[i,'Crew_invites_to_boat'] = 2 * (4-test_data.loc[i,'Pclass'])
    
    elif test_data.loc[i,'Res_category'] =='male':
        test_data.loc[i,'Crew_invites_to_boat'] = 1 * (4-test_data.loc[i,'Pclass'])


# selecting features
features = ["Crew_invites_to_boat","Age_void","SibSp", "Parch","Embarked"] -  0.77033
features = ["Pclass","Crew_invites_to_boat","Age_void","SibSp", "Parch","Embarked"]- 0.77511
features = ["Pclass","Crew_invites_to_boat","Sex","SibSp", "Parch","Embarked"] -0.77033
features = ["Pclass","Crew_invites_to_boat","Age_void","SibSp", "Parch","Embarked"]- 0.76555
features = ["Pclass","Crew_invites_to_boat","Embarked"] - 0.78947 rf 5 0.78947 rf4



